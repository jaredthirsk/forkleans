# Detailed Hierarchical Task List for Benchmarking

- [x] **Preparation Phase**
  - [x] Review current Granville implementation
    - [x] Build and run the forkleans repo locally.
    - [x] Test basic UDP RPC calls with LiteNetLib and Ruffles.
    - [x] Verify swappable transport functionality.
    - [x] Document any existing performance notes from granville/docs.
  - [x] Define benchmark scope
    - [x] List key metrics: latency (avg, p50, p95, p99), throughput, CPU/memory usage, packet loss recovery time, error rates.
    - [x] Identify comparison baselines: Granville UDP (LiteNetLib), Granville UDP (Ruffles), standard Orleans TCP.
    - [x] Specify configurations: reliable vs. unreliable modes, message sizes (small: 100B, medium: 1KB), frequencies (10Hz, 60Hz, 120Hz).
  - [x] Select tools and environment
    - [x] Choose benchmarking framework: BenchmarkDotNet for micro-benchmarks, custom simulator for end-to-end.
    - [x] Set up test environments: local machine, LAN cluster (2-5 machines), cloud VMs.
    - [x] Install network emulation tools (e.g., tc for Linux to simulate loss/latency).
    - [x] Prepare monitoring: Use dotnet-trace, PerfCollect, or integrate with Prometheus.

- [x] **Workload Design Phase**
  - [x] Create game-like workloads
    - [x] Develop client simulator: Generate RPC calls mimicking player actions (e.g., position updates, chat messages, entity spawns).
    - [x] Implement server zones: Single-zone, multi-zone with inter-server RPC.
    - [x] Add variability: Ramp-up connections (100 to 10,000), burst traffic.
  - [x] Integrate instrumentation
    - [x] Add timers/metrics to RPC send/receive paths in Granville code.
    - [x] Hook into UDP libraries for low-level stats (e.g., packets sent/lost).
    - [x] Ensure optional silo coordination is testable.

- [x] **Implementation Phase**
  - [x] Build benchmark harness
    - [x] Create scripts to automate runs: Configure transport, workload, and impairments.
    - [x] Implement data collection: Log to CSV/JSON for analysis.
    - [x] Develop visualization scripts: Use Python/Matplotlib or Excel for graphs.
  - [x] Handle edge cases
    - [x] Test with induced network issues: 1-10% packet loss, 10-100ms added latency, jitter.
    - [x] Evaluate reliability layers: Retries, acknowledgments for critical messages.

- [x] **Execution Phase**
  - [x] Run micro-benchmarks
    - [x] Single RPC call latency: 1000 iterations per config.
    - [x] Throughput under load: Fixed clients sending continuously.
  - [x] Run end-to-end simulations
    - [x] Game scenario 1: FPS-style (high-frequency updates, low reliability).
    - [x] Game scenario 2: MOBA-style (mixed events, some reliable commands).
    - [x] Scale tests: Increase clients/connections until failure.
    - [x] Collect baselines
    - [x] Repeat all with standard Orleans (disable Granville extensions).
    - [x] Compare UDP transports head-to-head.
  - [x] Perform statistical validation
    - [x] Run each test 10+ times, compute means/SD/confidence intervals.
    - [x] Use t-tests or similar for significant differences.

- [x] **Analysis and Reporting Phase**
  - [x] Analyze results
    - [x] Create comparison tables: Metrics by config (e.g., UDP vs TCP latency reduction %).
    - [x] Identify wins/losses (e.g., UDP better for latency but higher errors).
    - [x] Note impacts for games: Suitability for real-time vs. turn-based.
  - [x] Generate reports
    - [x] Write summary doc: Key findings, graphs, raw data appendix.
    - [x] Add recommendations: Optimizations, security integrations (e.g., auth over UDP).
  - [x] Iterate if needed
    - [x] Fix any discovered bugs during testing.
    - [x] Re-run targeted benchmarks post-fixes.